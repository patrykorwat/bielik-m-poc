# ğŸš€ Przewodnik MLX - Apple Silicon Optimized Inference

## Co to jest MLX?

MLX to framework machine learning zaprojektowany przez Apple specjalnie dla ukÅ‚adÃ³w Apple Silicon. Wykorzystuje Neural Engine i GPU do ultraszybkiego lokalnego inference bez kosztÃ³w API.

## Instalacja MLX

### Opcja 1: Homebrew (zalecane)

```bash
brew install mlx-lm
```

### Opcja 2: pip

```bash
pip install mlx mlx-lm
```

## Uruchamianie serwera MLX

### Podstawowe uÅ¼ycie

```bash
mlx_lm.server --model mlx-community/Llama-3.2-3B-Instruct-4bit
```

Serwer uruchomi siÄ™ na `http://localhost:8080` (domyÅ›lnie).

### Zaawansowane opcje

```bash
# Niestandardowy port
mlx_lm.server --model mlx-community/Llama-3.2-3B-Instruct-4bit --port 8000

# WiÄ™kszy model (wymaga wiÄ™cej RAM)
mlx_lm.server --model mlx-community/Llama-3.2-7B-Instruct-4bit

# Polski model Bielik
mlx_lm.server --model mlx-community/Bielik-11B-v2.3-Instruct-4bit
```

## DostÄ™pne modele MLX

### Modele maÅ‚e (3B) - szybkie, wymagajÄ… ~4GB RAM
- `mlx-community/Llama-3.2-3B-Instruct-4bit`
- `mlx-community/Phi-3-mini-4k-instruct-4bit`

### Modele Å›rednie (7B-11B) - dobra jakoÅ›Ä‡, wymagajÄ… ~8GB RAM
- `mlx-community/Llama-3.2-7B-Instruct-4bit`
- `mlx-community/Mistral-7B-Instruct-v0.3-4bit`
- `mlx-community/Bielik-11B-v2.3-Instruct-4bit` â­ Polski model!

### Modele duÅ¼e (13B+) - najlepsza jakoÅ›Ä‡, wymagajÄ… 16GB+ RAM
- `mlx-community/Mixtral-8x7B-Instruct-v0.1-4bit`

## Konfiguracja w Bielik-M

### W interfejsie uÅ¼ytkownika

1. Wybierz "MLX (Apple Silicon - lokalny)" z dropdown
2. WprowadÅº URL serwera (domyÅ›lnie `http://localhost:8080`)
3. WprowadÅº nazwÄ™ modelu (np. `mlx-community/Llama-3.2-3B-Instruct-4bit`)
4. Kliknij "Rozpocznij"

### Programatycznie

```typescript
import { GroupChatOrchestrator, createMathAgents } from './services/agentService';

const agents = createMathAgents();

const orchestrator = new GroupChatOrchestrator(
  'mlx',
  agents,
  undefined,
  {
    baseUrl: 'http://localhost:8080',
    model: 'mlx-community/Llama-3.2-3B-Instruct-4bit',
    temperature: 0.7,
    maxTokens: 4096
  }
);

// UÅ¼yj orchestratora
await orchestrator.orchestrateConversation(
  "RozwiÄ…Å¼ rÃ³wnanie: xÂ² + 5x + 6 = 0",
  2,
  (message) => console.log(message)
);
```

## Zalety MLX

âœ… **Darmowy** - brak kosztÃ³w API
âœ… **Prywatny** - wszystkie dane pozostajÄ… lokalnie
âœ… **Szybki** - akceleracja sprzÄ™towa Neural Engine
âœ… **Offline** - dziaÅ‚a bez poÅ‚Ä…czenia z internetem
âœ… **Efektywny** - optymalizacja dla Apple Silicon

## Wymagania systemowe

- Mac z Apple Silicon (M1, M2, M3, M4, M5)
- macOS 14.0 lub wyÅ¼szy
- 8GB RAM (min.), 16GB RAM (zalecane)
- ~10GB wolnego miejsca na dysku (dla modelu)

## RozwiÄ…zywanie problemÃ³w

### Serwer siÄ™ nie uruchamia

```bash
# SprawdÅº, czy MLX jest zainstalowane
mlx_lm.server --help

# JeÅ›li nie dziaÅ‚a, przeinstaluj
pip uninstall mlx mlx-lm
pip install mlx mlx-lm
```

### BÅ‚Ä…d poÅ‚Ä…czenia

1. Upewnij siÄ™, Å¼e serwer MLX dziaÅ‚a:
   ```bash
   curl http://localhost:8080/v1/models
   ```

2. SprawdÅº poprawny port w konfiguracji

### Model pobiera siÄ™ zbyt dÅ‚ugo

Pierwsze uruchomienie pobiera model (~4-8GB). To normalne. Kolejne uruchomienia bÄ™dÄ… natychmiastowe.

### Za maÅ‚o pamiÄ™ci

UÅ¼yj mniejszego modelu:
```bash
mlx_lm.server --model mlx-community/Llama-3.2-3B-Instruct-4bit
```

## PorÃ³wnanie z Claude

| Aspekt | MLX | Claude |
|--------|-----|--------|
| Koszt | ğŸ’° Darmowy | ğŸ’°ğŸ’°ğŸ’° ~$0.003/1K tokens |
| JakoÅ›Ä‡ odpowiedzi | â­â­â­â­ | â­â­â­â­â­ |
| SzybkoÅ›Ä‡ | âš¡âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡âš¡ |
| PrywatnoÅ›Ä‡ | ğŸ”’ 100% lokalne | â˜ï¸ Cloud |
| Offline | âœ… Tak | âŒ Nie |
| Setup | ğŸ”§ Wymaga instalacji | ğŸ”‘ Tylko klucz API |

## Najlepsze praktyki

1. **WybÃ³r modelu**: Zacznij od 3B dla testÃ³w, potem skaluj w gÃ³rÄ™
2. **Temperature**: 0.7 dla kreatywnych odpowiedzi, 0.3 dla precyzyjnych
3. **Max tokens**: 4096 to dobry balans (wiÄ™cej = wolniej)
4. **Rundy**: 2 rundy wystarczÄ… dla wiÄ™kszoÅ›ci zadaÅ„

## Dodatkowe zasoby

- [MLX Documentation](https://ml-explore.github.io/mlx/)
- [MLX Community Models](https://huggingface.co/mlx-community)
- [Apple ML Research](https://machinelearning.apple.com/)

## Wsparcie

W razie problemÃ³w:
1. SprawdÅº logi serwera MLX
2. Upewnij siÄ™, Å¼e masz najnowszÄ… wersjÄ™ MLX
3. ZgÅ‚oÅ› issue na GitHub projektu bielik-m-poc
