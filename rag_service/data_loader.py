"""Åadowanie i chunkowanie danych z 3 ÅºrÃ³deÅ‚ dla RAG."""
import json
import re
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass, field, asdict

from config import METHODS_JSON, INFORMATOR_MD, INFORMATOR_PDF_CHUNKS, DATASETS_DIR


@dataclass
class Chunk:
    """Pojedynczy chunk wiedzy dla RAG."""
    id: str
    source: str           # "methods" | "informator" | "dataset"
    category: str         # kategoria tematyczna
    title: str            # tytuÅ‚/nazwa
    content: str          # peÅ‚na treÅ›Ä‡ do wyszukiwania
    sympy_hint: str = ""  # podpowiedÅº SymPy
    tips: str = ""        # wskazÃ³wki dla agenta 11B
    exam_problems: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_search_text(self) -> str:
        """Tekst do indeksowania TF-IDF."""
        parts = [self.title, self.content]
        if self.sympy_hint:
            parts.append(self.sympy_hint)
        if self.tips:
            parts.append(self.tips)
        if self.category:
            parts.append(self.category)
        return " ".join(parts)

    def to_dict(self) -> dict:
        return asdict(self)


def load_methods_chunks() -> List[Chunk]:
    """Åaduj chunki z mathematical_methods.json â€” 1 chunk per metoda."""
    chunks = []

    with open(METHODS_JSON, "r", encoding="utf-8") as f:
        data = json.load(f)

    for cat in data.get("categories", []):
        cat_id = cat["id"]
        cat_name = cat.get("name", cat_id)

        for method in cat.get("methods", []):
            method_id = method["id"]

            # Buduj treÅ›Ä‡ chunka
            content_parts = [
                method.get("description", ""),
                f"Kiedy uÅ¼ywaÄ‡: {method.get('when_to_use', '')}",
            ]
            if method.get("exam_problems"):
                content_parts.append(f"Zadania egzaminacyjne: {', '.join(method['exam_problems'])}")

            chunk = Chunk(
                id=f"method_{cat_id}_{method_id}",
                source="methods",
                category=cat_name,
                title=f"{method.get('name', method_id)} ({method.get('name_en', '')})",
                content=" ".join(content_parts),
                sympy_hint=method.get("sympy_approach", ""),
                tips=method.get("tips_for_11b", ""),
                exam_problems=method.get("exam_problems", []),
                metadata={
                    "sympy_functions": method.get("sympy_functions", []),
                    "category_id": cat_id,
                    "method_id": method_id,
                }
            )
            chunks.append(chunk)

    # Dodaj teÅ¼ mapowania typ_zadania â†’ metody
    problem_map = data.get("problem_type_to_method_map", {})
    for problem_type, method_ids in problem_map.items():
        readable = problem_type.replace("_", " ")
        chunk = Chunk(
            id=f"mapping_{problem_type}",
            source="methods",
            category="mapowanie",
            title=f"Typ zadania: {readable}",
            content=f"Dla zadania typu '{readable}' uÅ¼yj metod: {', '.join(method_ids)}",
            metadata={"problem_type": problem_type, "method_ids": method_ids}
        )
        chunks.append(chunk)

    # Dodaj cheatsheet jako jeden chunk
    cheatsheet = data.get("sympy_cheatsheet", {})
    if cheatsheet:
        cs_lines = [f"{k}: {v}" for k, v in cheatsheet.items()]
        chunk = Chunk(
            id="sympy_cheatsheet",
            source="methods",
            category="SymPy",
            title="SymPy Cheatsheet - najwaÅ¼niejsze komendy",
            content="\n".join(cs_lines),
            sympy_hint="\n".join(cs_lines),
        )
        chunks.append(chunk)

    return chunks


def load_informator_chunks() -> List[Chunk]:
    """Åaduj chunki z informator_analysis.md â€” 1 chunk per zadanie/sekcja."""
    chunks = []

    if not INFORMATOR_MD.exists():
        print(f"âš ï¸  Brak pliku {INFORMATOR_MD}")
        return chunks

    text = INFORMATOR_MD.read_text(encoding="utf-8")

    # Podziel na sekcje po nagÅ‚Ã³wkach zadaÅ„ (#### **Zadanie X.**)
    task_pattern = re.compile(
        r'####\s+\*\*Zadanie\s+(\d+)\.\s*\(([^)]+)\)\*\*\s*\n(.*?)(?=####\s+\*\*Zadanie|\n##\s|\Z)',
        re.DOTALL
    )

    for match in task_pattern.finditer(text):
        task_num = match.group(1)
        points = match.group(2)
        body = match.group(3).strip()

        # WyciÄ…gnij kategorie z body
        topic_match = re.search(r'\*\*Topic\*\*:\s*(.+)', body)
        topic = topic_match.group(1).strip() if topic_match else ""

        skills_match = re.search(r'\*\*Math Skills Needed\*\*:\s*(.+?)(?=\n\*\*|\Z)', body, re.DOTALL)
        skills = skills_match.group(1).strip() if skills_match else ""

        sympy_match = re.search(r'\*\*SymPy.*?\*\*:\s*(.+?)(?=\n\*\*|\Z)', body, re.DOTALL)
        sympy_info = sympy_match.group(1).strip() if sympy_match else ""

        complexity_match = re.search(r'\*\*Computational Complexity\*\*:\s*(.+)', body)
        complexity = complexity_match.group(1).strip() if complexity_match else ""

        chunk = Chunk(
            id=f"informator_zadanie_{task_num}",
            source="informator",
            category=topic or "matura",
            title=f"Zadanie {task_num} ({points}) - {topic}",
            content=body[:800],  # Ogranicz rozmiar
            sympy_hint=sympy_info,
            tips=f"ZÅ‚oÅ¼onoÅ›Ä‡: {complexity}. UmiejÄ™tnoÅ›ci: {skills}",
            metadata={
                "task_number": int(task_num),
                "points": points,
                "topic": topic,
            }
        )
        chunks.append(chunk)

    # Dodaj sekcje ogÃ³lne (Exam Structure, etc.)
    section_pattern = re.compile(
        r'##\s+(\d+)\.\s+(.+?)\n(.*?)(?=\n##\s+\d|\Z)',
        re.DOTALL
    )

    for match in section_pattern.finditer(text):
        sec_num = match.group(1)
        sec_title = match.group(2).strip()
        sec_body = match.group(3).strip()

        # Pomijamy sekcjÄ™ z zadaniami (juÅ¼ przetworzona)
        if "EXAMPLE PROBLEMS" in sec_title.upper():
            continue

        if len(sec_body) > 100:  # Tylko niepuste sekcje
            chunk = Chunk(
                id=f"informator_section_{sec_num}",
                source="informator",
                category="struktura_egzaminu",
                title=sec_title,
                content=sec_body[:1000],
                metadata={"section_number": int(sec_num)}
            )
            chunks.append(chunk)

    return chunks


def load_dataset_chunks() -> List[Chunk]:
    """Åaduj chunki z historycznych zestawÃ³w maturalnych (datasets/*.json)."""
    chunks = []

    if not DATASETS_DIR.exists():
        print(f"âš ï¸  Brak katalogu {DATASETS_DIR}")
        return chunks

    for json_file in sorted(DATASETS_DIR.glob("*.json")):
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                tasks = json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            print(f"âš ï¸  BÅ‚Ä…d Å‚adowania {json_file}: {e}")
            continue

        year = json_file.stem.split("_")[0]

        for task in tasks:
            meta = task.get("metadata", {})
            task_num = meta.get("task_number", 0)
            max_points = meta.get("max_points", 1)

            question = task.get("question", "")
            answer = task.get("answer", "")
            options = task.get("options", {})

            # Buduj treÅ›Ä‡
            content = question
            if options:
                opts_str = "; ".join(f"{k}) {v}" for k, v in options.items())
                content += f" Opcje: {opts_str}"
            content += f" OdpowiedÅº: {answer}"

            # Kategoryzuj na podstawie treÅ›ci
            category = _infer_category(question)

            chunk = Chunk(
                id=f"dataset_{year}_{task_num}",
                source="dataset",
                category=category,
                title=f"Matura {year}, Zadanie {task_num} ({max_points}pkt)",
                content=content,
                metadata={
                    "year": int(year),
                    "task_number": task_num,
                    "max_points": max_points,
                    "has_options": bool(options),
                    "answer": answer,
                }
            )
            chunks.append(chunk)

    return chunks


def _infer_category(question: str) -> str:
    """Heurystyczna kategoryzacja zadania na podstawie treÅ›ci."""
    q = question.lower()

    # KolejnoÅ›Ä‡ ma znaczenie - bardziej specyficzne najpierw
    if any(w in q for w in ["prawdopodobie", "losow", "rzut", "kuli", "urna"]):
        return "prawdopodobieÅ„stwo"
    if any(w in q for w in ["kombinacj", "permutacj", "wariacj", "na ile sposob"]):
        return "kombinatoryka"
    if any(w in q for w in ["graniastosÅ‚up", "ostrosÅ‚up", "stoÅ¼ek", "walec", "kula", "sferze", "bryÅ‚y"]):
        return "stereometria"
    if any(w in q for w in ["sin", "cos", "tg", "ctg", "trygonometr", "kÄ…t"]):
        return "trygonometria"
    if any(w in q for w in ["pochodn", "caÅ‚k", "granica", "ciÄ…gÅ‚o"]):
        return "rachunek rÃ³Å¼niczkowy"
    if any(w in q for w in ["prosta", "okrÄ…g", "wspÃ³Å‚rzÄ™dn", "wektor", "odlegÅ‚oÅ›Ä‡ punkt"]):
        return "geometria analityczna"
    if any(w in q for w in ["trÃ³jkÄ…t", "czworokÄ…t", "pole", "obwÃ³d", "rÃ³wnolegÅ‚", "prostokÄ…t"]):
        return "geometria pÅ‚aska"
    if any(w in q for w in ["ciÄ…g", "arytmetycz", "geometrycz", "a_n", "suma.*wyraz"]):
        return "ciÄ…gi"
    if any(w in q for w in ["funkcj", "wykres", "dziedzin", "monotoniczn", "f(x)"]):
        return "funkcje"
    if any(w in q for w in ["log", "logarytm"]):
        return "logarytmy"
    if any(w in q for w in ["wielomian", "stopni", "pierwiastk"]):
        return "wielomiany"
    if any(w in q for w in ["rÃ³wnani", "rozwi", "nierÃ³wnoÅ›"]):
        return "rÃ³wnania i nierÃ³wnoÅ›ci"
    if any(w in q for w in ["procent", "zysk", "strat", "cen"]):
        return "procenty i zastosowania"

    return "algebra"


def load_pdf_chunks() -> List[Chunk]:
    """Åaduj chunki wyekstrahowane z Informator PDF (zadania, rozwiÄ…zania, kryteria, wzory)."""
    chunks = []

    if not INFORMATOR_PDF_CHUNKS.exists():
        print(f"âš ï¸  Brak pliku {INFORMATOR_PDF_CHUNKS}")
        return chunks

    with open(INFORMATOR_PDF_CHUNKS, "r", encoding="utf-8") as f:
        data = json.load(f)

    for item in data.get("chunks", []):
        chunk = Chunk(
            id=item["id"],
            source="informator_pdf",
            category=item.get("category", "matura_rozszerzona"),
            title=item.get("title", ""),
            content=item.get("content", ""),
            sympy_hint=item.get("sympy_hint", ""),
            tips=item.get("tips", ""),
            metadata=item.get("metadata", {}),
        )
        chunks.append(chunk)

    return chunks


def load_all_chunks() -> List[Chunk]:
    """Åaduj wszystkie chunki ze wszystkich ÅºrÃ³deÅ‚."""
    all_chunks = []

    print("ğŸ“š Åadowanie mathematical_methods.json...")
    methods = load_methods_chunks()
    print(f"   â†’ {len(methods)} chunkÃ³w z metod")
    all_chunks.extend(methods)

    print("ğŸ“š Åadowanie informator_analysis.md...")
    informator = load_informator_chunks()
    print(f"   â†’ {len(informator)} chunkÃ³w z informatora")
    all_chunks.extend(informator)

    print("ğŸ“š Åadowanie informator_pdf_chunks.json...")
    pdf_chunks = load_pdf_chunks()
    print(f"   â†’ {len(pdf_chunks)} chunkÃ³w z PDF informatora")
    all_chunks.extend(pdf_chunks)

    print("ğŸ“š Åadowanie datasets/*.json...")
    datasets = load_dataset_chunks()
    print(f"   â†’ {len(datasets)} chunkÃ³w z datasetÃ³w")
    all_chunks.extend(datasets)

    print(f"\nâœ… ÅÄ…cznie: {len(all_chunks)} chunkÃ³w")
    return all_chunks


if __name__ == "__main__":
    chunks = load_all_chunks()
    for c in chunks[:5]:
        print(f"\n--- {c.id} [{c.source}] ---")
        print(f"  TytuÅ‚: {c.title}")
        print(f"  Kategoria: {c.category}")
        print(f"  TreÅ›Ä‡: {c.content[:120]}...")
